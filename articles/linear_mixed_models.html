<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Linear Mixed Models • FamModel</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Linear Mixed Models">
<meta property="og:description" content="FamModel">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">FamModel</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/linear_mixed_models.html">Linear Mixed Models</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="linear_mixed_models_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Linear Mixed Models</h1>
            
      
      
      <div class="hidden name"><code>linear_mixed_models.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Linear mixed models can be fit by the <a href="../reference/FamData.html#method-lmm"><code>FamData$lmm()</code></a> method, which produces a <a href="../reference/FamLMMFit.html"><code>FamLMMFit</code></a> object inheriting from the class <a href="../reference/FamModelFit.html"><code>FamModelFit</code></a>. These models assume the standard multivariate normal measured genotype model <span class="citation">(Boerwinkle, Chakraborty, and Sing 1986; Lange 2002)</span> holds for each family in the population. The current implementation of the model assumes an additive polygenic effect, parameterized in terms of heritability, and no shared environmental effect. The heritability and total variance can be allowed to vary across groups of families, but, within each group, all founders in the family are assumed to be drawn from the same randomly mating population. The current implementation assumes that each family has been ascertained through a single proband meeting certain criteria, in which case the appropriate multivariate normal likelihood conditions on the observed data in the proband <span class="citation">(see Hopper and Mathews 1982; Beaty, Liang, and Rao 1987)</span>. Families without probands or more than one proband are therefore excluded.</p>
</div>
<div id="likelihood" class="section level2">
<h2 class="hasAnchor">
<a href="#likelihood" class="anchor"></a>Likelihood</h2>
<p>Let <span class="math inline">\(\beta\)</span> denote a vector of parameters of the mean model, <span class="math inline">\(h^2_{a, q_i}\)</span> denote the narrow-sense heritability for a randomly mating population <span class="math inline">\(q_i\)</span>, and <span class="math inline">\(\sigma_{q_i}\)</span> denote the square root of the total trait variance in this population. When convenient, these will be referred to collectively by <span class="math inline">\(\theta = \left\{\beta, h^2_a, \sigma\right\}\)</span>, where <span class="math inline">\(h^2_a\)</span> and <span class="math inline">\(\sigma\)</span> are the sets of population-specific parameters necessary to describe the populations of all families in the model. For a family <span class="math inline">\(i\)</span> randomly selected from the population, the joint distribution of the quantitative traits, <span class="math inline">\(\mathbf{y}_i\)</span>, conditional on the variables <span class="math inline">\(\mathbf{X}_i\)</span> and the randomly mating population <span class="math inline">\(q_i\)</span> to which all family founders belong is:</p>
<p><span class="math display">\[\begin{gather}
\mathbf{y}_i | \mathbf{X}_i, q_i, \Phi_i; \theta \sim
  MVN \left( \mu_i, \Sigma_i \right) \\
\mu_i = \mathbf{X}_i\beta \\
\Sigma_i = 2\Phi_i h^2_{a, q_i} \sigma^2_{q_i} +
  \mathbf{I} \left( 1 - h^2_{a, q_i} \right) \sigma^2_{q_i}
\label{eq:popdist}
\tag{1}
\end{gather}\]</span></p>
<p>where <span class="math inline">\(MVN\left(\mu, \Sigma\right)\)</span> denotes the multivariate normal distribution with mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Phi_i\)</span> is the kinship matrix for the family structure.</p>
<p>When a family is ascertained through a proband meeting certain conditions, valid inferences for the population parameters <span class="math inline">\(\theta\)</span> can be obtained by using the distribution of the quantitative traits in family members conditional on the proband’s <span class="citation">(see Hopper and Mathews 1982; Beaty, Liang, and Rao 1987)</span>. Let <span class="math inline">\(m_i\)</span> be the number of individuals in family <span class="math inline">\(i\)</span> included in the model, <span class="math inline">\(j_i \in \{1, ..., m_i\}\)</span> be the index of the proband, and <span class="math inline">\((-j_i)\)</span> denote the indexes of all non-proband individuals in their original order. For example, if <span class="math inline">\(j_i = 3\)</span>, we have <span class="math inline">\(\mathbf{y}_{i (-j_i)} = \left[y_{i1}, y_{i2}, y_{i4}, ..., y_{i m_i}\right]^{'}\)</span>. Using this notation, we can write the appropriate conditional distribution as:</p>
<p><span class="math display">\[\begin{gather}
\mathbf{y}_{i(-j_i)} | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta \sim
  MVN\left(\eta_i, \Omega_i \right) \\
\eta_i = \mathbf{X}_{i (-j_i)}\beta +
  \Sigma_{i(-j_i)j_i} \Sigma_{i j_i j_i}^{-1}
  \left(y_{i j_i} - \mathbf{x}_{i j_i} \beta \right) \\
\Omega_i = \Sigma_{i(-j_i)(-j_i)} -
  \Sigma_{i(-j_i)j_i} \Sigma_{i j_i j_i}^{-1} \Sigma_{i j_i (-j_i)}
\tag{2}
\label{eq:conddist}
\end{gather}\]</span></p>
<p>To avoid confusion with the skipped index <span class="math inline">\(j_i\)</span>, we introduce a new index <span class="math inline">\(k \in \{1, ..., m_i - 1\}\)</span> that numbers non-proband individuals in family <span class="math inline">\(i\)</span> in their order of appearance in <span class="math inline">\(\mathbf{y}_{i(-j_i)}\)</span>, <span class="math inline">\(\eta_i\)</span>, and <span class="math inline">\(\Omega_i\)</span>.</p>
<p>Provided that the population of families is essentially infinite, the probability of the same individual being sampled in two families is zero, and the conditional probability of the sample of <span class="math inline">\(N\)</span> families is the product of the conditional densities from (\ref{eq:conddist}). Denoting each density from (\ref{eq:conddist}) by <span class="math inline">\(f\left(\mathbf{y}_{i(-j_i)} | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta\right)\)</span>, the conditional likelihood is:</p>
<p><span class="math display">\[\begin{equation}
L\left(\theta\right)
  = f\left(\mathbf{y}_{\mathrm{np}} | \mathbf{y}_{\mathrm{p}}, \mathbf{X},
    \mathbf{q}, \Phi; \theta \right)
  = \prod_{i=1}^N f\left(\mathbf{y}_{i(-j_i)} | y_{i j_i}, \mathbf{X}_i, q_i,
    \Phi_i ; \theta \right)
\tag{3}
\label{eq:condlik}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{y}_{\mathrm{np}} = \{\mathbf{y}_{1(-j_1)}, ..., \mathbf{y}_{N(-j_N)}\} = \{\mathbf{y}_{\mathrm{np},1}, ..., \mathbf{y}_{\mathrm{np},N}\}\)</span>, <span class="math inline">\(\mathbf{y}_{\mathrm{p}} = \{y_{1 j_1}, ..., y_{N j_N}\}\)</span>, <span class="math inline">\(\mathbf{X} = \{\mathbf{X}_1, ..., \mathbf{X}_N\}\)</span>, <span class="math inline">\(\mathbf{q} = \{q_1, ..., q_N\}\)</span>, and <span class="math inline">\(\Phi = \{\Phi_1, ..., \Phi_N\}\)</span>.</p>
</div>
<div id="optimization" class="section level2">
<h2 class="hasAnchor">
<a href="#optimization" class="anchor"></a>Optimization</h2>
<p>From the definition of conditional probability, we can restate the conditional likelihood in (\ref{eq:condlik}) in a more convenient form for optimization. Letting <span class="math inline">\(f\left(\mathbf{y}_{i} | \mathbf{X}_i, q_i, \Phi_i; \theta \right)\)</span> denote the joint density from (\ref{eq:popdist}) and <span class="math inline">\(f\left(y_{i j_i} | \mathbf{x}_{i j_i}, q_i, \Phi_{i j_i j_i}; \theta \right)\)</span> denote the marginal <span class="math inline">\(N\left(\mathbf{x}_{i j_i} \beta, \Sigma_{i j_i j_i}\right)\)</span> density of <span class="math inline">\(y_{i j_i}\)</span> obtained from this joint density, our objective is to maximize the loglikelihood:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
l\left(\theta\right) &amp;=
  \sum_{i=1}^N \ln f\left(\mathbf{y}_{i(-j_i)} | y_{i j_i}, \mathbf{X}_i, q_i,
  \Phi_i, \theta \right) \\
&amp;= \sum_{i=1}^{N} \left(
    \ln f\left(\mathbf{y}_{i} | \mathbf{X}_i, q_i, \Phi_i; \theta\right) -
    \ln f\left(
      y_{i j_i} | \mathbf{x}_{i j_i}, q_i, \Phi_{i j_i j_i}; \theta
    \right)
  \right)
\end{split}
\tag{4}
\label{eq:loglik}
\end{equation}\]</span></p>
<p>with respect to <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(g(\theta)\)</span> and <span class="math inline">\(H(\theta)\)</span> be the analytic gradient and Hessian of the loglikelihood (\ref{eq:loglik}) evaluated at <span class="math inline">\(\theta\)</span>. The <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> L-BFGS-B optimizer is used to minimize the <em>negative</em> of (\ref{eq:loglik}) with analytic <span class="math inline">\(-g(\theta)\)</span> calculated rapidly using the <code>TMB</code> package. The <span class="math inline">\(\beta\)</span> parameters in <span class="math inline">\(\theta\)</span> are unconstrained, but the <span class="math inline">\(h^2_a\)</span> and <span class="math inline">\(\sigma\)</span> parameters are constrained to lie within <span class="math inline">\([0,1]\)</span> and <span class="math inline">\((0, \infty)\)</span>, respectively. Note that <span class="math inline">\(\sigma\)</span>, rather than <span class="math inline">\(\sigma^2\)</span>, is used to make its scale more comparable with the <span class="math inline">\(\beta\)</span> parameters and avoid an ill-conditioned Hessian.</p>
<p>To minimize potential numerical issues, we use an affine transformation of the parameters <span class="math inline">\(\theta\)</span> by a diagonal matrix <span class="math inline">\(D\)</span> and perform optimization on <span class="math inline">\(D\theta\)</span>. The matrix <span class="math inline">\(D\)</span> is the chosen so that the diagonal elements of the Hessian approximation in the L-BFGS-B algorithm will be approximately 1, as suggested by <span class="citation">Fletcher (2000, 59)</span>. In particular, <span class="math inline">\(D = \mathrm{diag}\left(\sqrt{-H_{11}(\theta^{(0)})}, ..., \sqrt{-H_{pp}(\theta^{(0)}}\right)\)</span>, where <span class="math inline">\(\mathrm{diag}\)</span> indicates a (block) diagonal matrix with zeros off the main (block) diagonal and <span class="math inline">\(\theta^{(0)}\)</span> is the vector of initial parameter estimates. This affine transformation should also yield a more spherical trust region in the transformed parameter space <span class="citation">(Nocedal and Wright 2006, 95–97)</span>. If any of the <span class="math inline">\(\sqrt{-H_{ii}(\theta^{(0)})}\)</span> are infinite or invalid, <span class="math inline">\(D\)</span> defaults to the identity matrix. Otherwise, any elements of <span class="math inline">\(D\)</span> that are <span class="math inline">\(&lt;\sqrt{\epsilon}\)</span>, where <span class="math inline">\(\epsilon\)</span> is the double-precision floating-point epsilon, are set to the minimum of other <span class="math inline">\(D\)</span> elements greater than or equal to this quantity. The objective function value is scaled by the inverse of its value at <span class="math inline">\(\theta^{(0)}\)</span> because the stopping criteria are not scale invariant <span class="citation">(Zhu et al. 1997)</span>. Default stopping criteria are a maximum absolute projected gradient element <span class="math inline">\(&lt;\sqrt{\epsilon}\)</span> or no change in the objective function between iterations (i.e., <code>factr = 0</code> in <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code>). These tolerances, but not scaling choices, can be overriden by the user.</p>
<p>The <a href="../reference/FamLMMFit.html#method-print"><code>FamLMMFit$print()</code></a> method provides optimization diagnostics, including the covergence status and message from <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code>. To identify potential issues with conditioning that might affect optimization or covariance matrix estimation, the smallest eigenvalue and recpirocal condition number of <span class="math inline">\(-H(\hat{\theta})\)</span> at the solution <span class="math inline">\(\hat{\theta}\)</span> are provided. Convergence measures are also provided, including the maximum absolute element of the loglikelihood gradient at the solution, <span class="math inline">\(\lVert g(\hat{\theta}) \rVert_\infty\)</span>, and the scaled gradient criterion <span class="math inline">\(-g(\hat{\theta})^{'} H(\hat{\theta})^{-1} g(\hat{\theta})\)</span> recommended by <span class="citation">Belsley (1980)</span>. The first of these is not invariant to parameter scaling, and both of these may differ substantially from zero when the solution is on the boundary for one or more parameters. However, it is worth noting that the maximum absolute <em>projected</em> gradient element is still <span class="math inline">\(&lt;\sqrt{\epsilon}\)</span> under these circumstances if the output indicates that this convergence criterion was satisfied.</p>
</div>
<div id="inference" class="section level2">
<h2 class="hasAnchor">
<a href="#inference" class="anchor"></a>Inference</h2>
<p>Because <span class="math inline">\(h^2_{a, q_i}\)</span> can have a true value on the boundary of the parameter space (i.e., zero), standard asymptotic results for maximum likelihood estimators may not always apply. Let <span class="math inline">\(\theta_0 \in \Theta \subset \mathbb{R}^p\)</span> denote the true value of the parameter vector, which may be on the boundary of the parameter space. Futher, let <span class="math inline">\(g_i(\theta)\)</span> and <span class="math inline">\(H_i(\theta)\)</span> denote the score and Hessian component for one family evaluated at <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(g(\theta) = \sum_{i=1}^N g_i(\theta)\)</span> and <span class="math inline">\(H(\theta) = \sum_{i=1}^N H_i(\theta)\)</span>. It immediately follows under the usual regularity conditions on the density <span class="math inline">\(f\left(\mathbf{y}_{i(-j_i)} | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta \right)\)</span> <span class="citation">(Greene 2003)</span> that:</p>
<p><span class="math display">\[\begin{gather}
\mathrm{E}\left[g_i(\theta_0) | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i;
  \theta_0 \right] = \mathbf{0} \\
\mathrm{Var}\left(g_i(\theta_0) | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i;
  \theta_0\right)=E\left[-H_i(\theta_0) | y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i;
  \theta_0 \right] =
I_i\left(\theta_0\right)
\end{gather}\]</span></p>
<p>As long as no <span class="math inline">\(I_i\left(\theta_0\right)\)</span> dominates the average and <span class="math inline">\(\underset{N \rightarrow \infty}{\lim} N^{-1} \sum_{i=1}^{N} I_i\left(\theta_0\right) = \bar{I}\left(\theta_0\right)\)</span>, a finite, positive-definite matrix, in the neighborhood of <span class="math inline">\(\theta_0\)</span> it follows that:</p>
<p><span class="math display">\[\begin{gather}
N^{-1/2} g(\theta_0) \overset{d}{\longrightarrow}
  MVN\left(\mathbf{0}, \bar{I}\left(\theta_0\right)\right) \\
N^{-1} H(\theta_0) \overset{p}{\longrightarrow} -\bar{I}\left(\theta_0\right)
\end{gather}\]</span></p>
<p>The first convergence follows from the Multivariate Lindeberg-Feller Central Limit Theorem, and the second from applying Chebyshev’s Weak Law of Large Numbers to each matrix component <span class="citation">(Greene 2003)</span>. If the expectation of the absolute value of the third derivative of the loglikelihood is <span class="math inline">\(O(N)\)</span>, then the regularity conditions of Self and Liang <span class="citation">(1987)</span> are satisfied and <span class="math inline">\(\hat{\theta} \overset{p}{\longrightarrow} \theta_0\)</span>, even if <span class="math inline">\(\theta_0\)</span> is on the boundary of the parameter space. Futhermore, in the neighborhood of <span class="math inline">\(\theta_0\)</span>, the loglikelihood ratio admits a quadratic Taylor series expansion equivalent to <span class="citation">(Self and Liang 1987)</span>:</p>
<p><span class="math display">\[\begin{equation}
l\left(\theta_0 + N^{-1/2} \lambda\right) - l\left(\theta_0\right) =
  \lambda^{'} N^{-1/2} g(\theta_0) -
  \frac{1}{2} \lambda^{'} \bar{I}\left(\theta_0\right) \lambda + o_p(1)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is in the local parameter space <span class="math inline">\(\Lambda_N = \sqrt{N}\left(\Theta - \theta_0\right)\)</span>. Based on this expansion, the sequence of models with increasing <span class="math inline">\(N\)</span> is locally asymptotically normal at <span class="math inline">\(\theta_0\)</span> <span class="citation">(Van der Vaart 2007)</span>. Assuming that <span class="math inline">\(\Lambda_N \rightarrow \Lambda\)</span>, a convex cone at the origin, as <span class="math inline">\(N \rightarrow \infty\)</span>, the asymptotic distribution of <span class="math inline">\(\sqrt{N}\left(\hat{\theta} - \theta_0\right)\)</span> is the same as that of <span class="math inline">\(\hat{\lambda}\)</span> obtained as <span class="citation">(Van der Vaart 2007, Theorem 7.12; Self and Liang 1987, Theorem 2)</span>:</p>
<p><span class="math display">\[\begin{gather}
\hat{\lambda} =
  \underset{\lambda \in \Lambda}{\arg\min}
  \left(\mathbf{z} - \lambda\right)^{'}
  \bar{I}\left(\theta_0\right) \left(\mathbf{z} - \lambda\right) \\
\mathbf{z} \sim MVN\left(\mathbf{0}, \bar{I}\left(\theta_0\right)^{-1}\right)
\end{gather}\]</span></p>
<p>When <span class="math inline">\(\theta_0\)</span> is an interior point of <span class="math inline">\(\Theta\)</span>, <span class="math inline">\(\Lambda = \mathbb{R}^p\)</span>, <span class="math inline">\(\hat{\lambda} = \mathbf{z}\)</span>, and the parameter vector has the usual asymptotic multivariate normal distribution. When some parameters are on the boundary, <span class="math inline">\(\Lambda\)</span> has dimensions that are <span class="math inline">\(\left[0, \infty\right)\)</span> or <span class="math inline">\(\left(-\infty, 0\right]\)</span>, <span class="math inline">\(\hat{\lambda} \neq \mathbf{z}\)</span>, and usual results do not apply.</p>
<p>Nonetheless, valid Wald inferences using standard procedures can still be obtained for subsets of parameters orthogonal to those that fall on the boundary. Suppose that <span class="math inline">\(\theta\)</span> can be partitioned into <span class="math inline">\(\left[\theta^{'}_1, \theta^{'}_2\right]^{'}\)</span> that are orthogonal in the sense that the average expected information at the true values is <span class="math inline">\(\bar{I}\left(\theta_0\right) = \mathrm{diag}\left(\bar{I}\left(\theta_{10}\right), \bar{I}\left(\theta_{20}\right)\right)\)</span>. Further suppose that <span class="math inline">\(\theta_{20}\)</span> may fall on the boundary of <span class="math inline">\(\Theta_2\)</span> but that <span class="math inline">\(\theta_{10}\)</span> is an interior point of <span class="math inline">\(\Theta_1\)</span>. From above, the asymptotic distribution is the same as that of <span class="math inline">\(\hat{\lambda}\)</span> obtained as:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\hat{\lambda} &amp;=
  \underset{
    \left(\lambda_1, \lambda_2\right) \in \Lambda_1 \times \Lambda_2
  }{\arg\min}
  \begin{bmatrix}
    \left(\mathbf{z}_1 - \lambda_1\right)^{'} &amp;
    \left(\mathbf{z}_2 - \lambda_2\right)^{'}
  \end{bmatrix}
  \begin{bmatrix}
    \bar{I}\left(\theta_{10}\right) &amp; \mathbf{0} \\
    \mathbf{0} &amp;   \bar{I}\left(\theta_{20}\right)
  \end{bmatrix}
  \begin{bmatrix}
    \mathbf{z}_1 - \lambda_1 \\ \mathbf{z}_2 - \lambda_2
  \end{bmatrix} \\
&amp;= \underset{
    \left(\lambda_1, \lambda_2\right) \in \Lambda_1 \times \Lambda_2
  }{\arg\min}
  \left[
    \left(\mathbf{z}_1 - \lambda_1\right)^{'}
    \bar{I}\left(\theta_{10}\right) \left(\mathbf{z}_1 - \lambda_1\right) +
    \left(\mathbf{z}_2 - \lambda_2\right)^{'}
    \bar{I}\left(\theta_{20}\right) \left(\mathbf{z}_2 - \lambda_2\right)
  \right]
\end{split}
\end{equation}\]</span></p>
<p>Because the term in brackets is the sum of two positive definite quadratic forms, the value of <span class="math inline">\(\lambda_1\)</span> in <span class="math inline">\(\Lambda_1 = \mathbb{R}^p\)</span> that minimizes the above quantity is <span class="math inline">\(\mathbf{z}_1\)</span> for any value of <span class="math inline">\(\lambda_2\)</span> that minimizes the second term, meaning that <span class="math inline">\(\hat{\lambda}_1 = \mathbf{z}_1\)</span> regardless of whether any parameter in <span class="math inline">\(\theta_{20}\)</span> is on the boundary of <span class="math inline">\(\Theta_2\)</span>. In this case, <span class="math inline">\(\hat{\theta}_1\)</span> has the usual asymptotic <span class="math inline">\(MVN\left(\theta_{10}, N^{-1} \bar{I}\left(\theta_{10}\right)^{-1} \right)\)</span> as long as <span class="math inline">\(\theta_{10}\)</span> is an interior point of <span class="math inline">\(\Theta_1\)</span>. As a result, Wald inferences for any set of parameters are asymptotically valid as long as the parameters of interest and all parameters that are not orthogonal to them have true values in the interior of the parameter space.</p>
<p>For Wald inference, the estimated covariance matrix of the parameter estimates is obtained from the observed information as <span class="math inline">\(\hat{V}(\hat{\theta}) = -H(\hat{\theta})^{-1}\)</span>, which is equivalent to estimating <span class="math inline">\(\bar{I}\left(\theta_{0}\right)\)</span> by <span class="math inline">\(-N^{-1} H(\hat{\theta})\)</span>. Wald tests and confidence intervals using the standard normal distribution are produced for the <span class="math inline">\(\beta\)</span> parameters by the <a href="../reference/FamLMMFit.html#method-print"><code>FamLMMFit$print()</code></a> method, and Wald tests and confidence intervals for general linear contrasts of the form <span class="math inline">\(L\theta - m = 0\)</span> can be obtained with the inherited <a href="../reference/FamModelFit.html#method-contrast"><code>FamLMMFit$contrast()</code></a> method, which produces a <a href="../reference/Contrast.html"><code>Contrast</code></a> object. Wald tests and confidence intervals for each row of <span class="math inline">\(L\theta - m\)</span> are calculated using the standard normal distribution, and an overall Wald chi-square test of the null hypothesis <span class="math inline">\(L\theta - m = 0\)</span> with df equal to <span class="math inline">\(\mathrm{rank}(L)\)</span> are provided. These results can be displayed with the <a href="../reference/Contrast.html#method-print"><code>Contrast$print()</code></a> method.</p>
<p>We now consider the validity of the likelihood ratio tests that each <span class="math inline">\(h^2_{a, q_i} = 0\)</span>. Consider testing the null that one component of <span class="math inline">\(\theta_1\)</span> is the boundary value zero against the null that it is greater than zero in the above scenario. The asymptotic representation of the likelihood ratio statistic <span class="math inline">\(T_{LR}\)</span> may be written as <span class="citation">(Self and Liang 1987; Van der Vaart 2007)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp;
  \underset{
    \left(\lambda_1, \lambda_2\right) \in \Lambda_{1, H_0} \times \Lambda_2
  }{\inf} &amp;
  \left[
    \left(\mathbf{z}_1 - \lambda_1\right)^{'}
    \bar{I}\left(\theta_{10}\right) \left(\mathbf{z}_1 - \lambda_1\right) +
    \left(\mathbf{z}_2 - \lambda_2\right)^{'}
    \bar{I}\left(\theta_{20}\right) \left(\mathbf{z}_2 - \lambda_2\right)
  \right] \\
&amp; &amp;-
  \underset{
    \left(\lambda_1, \lambda_2\right) \in \Lambda_{1} \times \Lambda_2
  }{\inf}
  \left[
    \left(\mathbf{z}_1 - \lambda_1\right)^{'}
    \bar{I}\left(\theta_{10}\right) \left(\mathbf{z}_1 - \lambda_1\right) +
    \left(\mathbf{z}_2 - \lambda_2\right)^{'}
    \bar{I}\left(\theta_{20}\right) \left(\mathbf{z}_2 - \lambda_2\right)
  \right] \\
&amp; &amp;=
  \underset{\lambda_1 \in \Lambda_{1, H_0}}{\inf}
  \left[
    \left(\mathbf{z}_1 - \lambda_1\right)^{'}
    \bar{I}\left(\theta_{10}\right) \left(\mathbf{z}_1 - \lambda_1\right)
  \right] -
  \underset{\lambda_1 \in \Lambda_1}{\inf}
  \left[
    \left(\mathbf{z}_1 - \lambda_1\right)^{'}
    \bar{I}\left(\theta_{10}\right) \left(\mathbf{z}_1 - \lambda_1\right)
  \right]
\end{split}
\end{equation}\]</span></p>
<p>The second equality follows from the facts that the infimum of the sum set is the sum of the infima of the constituent sets and the infimum over <span class="math inline">\(\Lambda_2\)</span> is the same under both the null and alternative hypotheses. Thus, we only need to consider the the parameters in <span class="math inline">\(\theta_1\)</span> to obtain the asymptotic distribution of the likelihood ratio statistic. As long as no parameters in <span class="math inline">\(\theta_{10}\)</span> <em>other than the single parameter of interest</em> have true values lying on the boundary <span class="math inline">\(\Theta_1\)</span> under the null or alternative, the same argument as for case 5 in Self and Liang <span class="citation">(1987)</span> shows that the likelihood ratio statistic will have an asymptotic distribution that is a 50:50 mixture of <span class="math inline">\(\chi^2_0\)</span> and <span class="math inline">\(\chi^2_1\)</span>. Notably, this will not be the correct asymptotic null distribution if other parameters in <span class="math inline">\(\theta_{01}\)</span> aside from the one of interest take values on the boundary of <span class="math inline">\(\Theta_1\)</span> <span class="citation">(Self and Liang 1987, case 8)</span>. Thus, the likelihood ratio test for the null hypothesis <span class="math inline">\(h^2_{a, q_i} = 0\)</span> is valid as long as all parameters that are not orthogonal to <span class="math inline">\(h^2_{a, q_i}\)</span> have true values in the interior of the parameter space.</p>
<p>Likelihood ratio tests that each <span class="math inline">\(h^2_{a, q_i} = 0\)</span> are performed automatically on the first call to the <a href="../reference/FamLMMFit.html#method-print"><code>FamLMMFit$print()</code></a>, which calls the <a href="../reference/FamLMMFit.html#method-get_h2_a_lrts"><code>FamLMMFit$get_h2_a_lrts()</code></a> method. The latter method can also be called directly to obtain printed output and/or a <code>data.table</code> for further manipulation. The same Hessian and convergence diagnostics described above are supplied for each constrained maximization under the null. The p-value for the likelihood ratio statistic is calculated from the appropriate mixture as <span class="math inline">\(\frac{1}{2}1[T_{LR} = 0] + \frac{1}{2}\mathrm{Pr}\left(\chi^2_1 \geq T_{LR}\right)\)</span>. After the first call to <a href="../reference/FamLMMFit.html#method-get_h2_a_lrts"><code>FamLMMFit$get_h2_a_lrts()</code></a>, the likelihood ratio test results are cached in the <a href="../reference/FamLMMFit.html"><code>FamLMMFit</code></a> object to avoid redundant evaluations.</p>
<p>Whether the assumptions required for valid Wald and likelihood ratio inference are tenable depends to a large degree on model parameterization. For example, if there are 2 populations and there are mean model parameters in common between them, it is likely that all parameter estimates are correlated to some degree. In such situations, one must assume that the true values of <span class="math inline">\(h^2_{a, q_i}\)</span> are non-zero for <em>all</em> populations for asymptotically valid Wald inference and for the populations other than the <span class="math inline">\(q_i\)</span> for which <span class="math inline">\(h^2_{a, q_i} = 0\)</span> is being testsed for an asymptotically valid likelihood ratio test. However, if all model parameters are population-specific, then the likelihood is separable into components for families from each population involving only parameters specific to that population, and the population-specific parameters are orthogonal. In this case, asymptotically valid Wald inference for each population depends only on assuming that the true value of <span class="math inline">\(h^2_{a, q_i}\)</span> is non-zero <em>for that population</em>, and the likelihood ratio p-value presented is always asymptotically valid.</p>
<p>It is important to note that the above discussion applies to the true values of the parameters, not their estimates. In fact, there is a non-zero probability that <span class="math inline">\(\hat{h}^2_{a, q_i} = 0\)</span> for true <span class="math inline">\(h^2_{a, q_i} &gt; 0\)</span> and <span class="math inline">\(\hat{h}^2_{a, q_i} &gt; 0\)</span> for true <span class="math inline">\(h^2_{a, q_i} = 0\)</span> in finite samples.</p>
</div>
<div id="residuals-and-diagnostics" class="section level2">
<h2 class="hasAnchor">
<a href="#residuals-and-diagnostics" class="anchor"></a>Residuals and Diagnostics</h2>
<p>A <code>data.table</code> containing individual observations along with a full complement of residuals and diagnostics described in greater detail below can be obtained using the <a href="../reference/FamLMMFit.html#method-get_model_res"><code>FamLMMFit$get_model_res()</code></a> method.</p>
<p>Regarding <span class="math inline">\(\{y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i\}\)</span> as fixed, each of the residuals and diagnostics described below can be defined by a family-specific, continuous, vector-valued function of <span class="math inline">\(\mathbf{y}_{\mathrm{np}, i}\)</span> and the parameter vector <span class="math inline">\(\theta\)</span>. This function is denoted generically for any of these quantities by <span class="math inline">\(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \theta)\)</span> when convenient to do so; otherwise, the functional dependence on <span class="math inline">\(\theta\)</span> will be suppressed in the notation for a particular residual or diagnostic for simplicity. Define the following residuals for non-proband members of family <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{r}_i &amp;= \mathbf{y}_{\mathrm{np}, i} - \eta_i \\
\mathbf{r}_{\mathrm{s}, i} &amp;= \mathbf{S}_i^{-1} \mathbf{r}_i \\
\mathbf{r}_{\mathrm{c}, i} &amp;= \mathbf{L}_i^{-1} \mathbf{r}_i
\end{split}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}_i = \left[\sqrt{\Omega_{ikk}}\right]_{k = 1}^{m_i - 1}\)</span> and <span class="math inline">\(\mathbf{L}_i\)</span> is the lower Cholesky factor of <span class="math inline">\(\Omega_i\)</span> such that <span class="math inline">\(\mathbf{L}_i \mathbf{L}_i^{'} = \Omega_i\)</span> <span class="citation">(Harville 1997)</span>. If the assumed model is correctly specified, we obtain that at <span class="math inline">\(\theta = \theta_0\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{r}_i \mid y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta_0 &amp;\sim
  MVN\left( \mathbf{0}, \Omega_i\right) \\
\mathbf{r}_{\mathrm{s}, i} \mid y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta_0
  &amp;\sim MVN\left( \mathbf{0}, \Psi_i\right) \\
\mathbf{r}_{\mathrm{c}, i} \mid y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta_0
  &amp;\sim MVN\left( \mathbf{0}, \mathbf{I} \right)
\end{split}
\end{equation}\]</span></p>
<p>and <span class="math inline">\(\Psi_i\)</span> is a correlation matrix. <span class="math inline">\(\mathbf{r}_{\mathrm{s}, i}\)</span> is the vector of Pearson-type residuals. Each element <span class="math inline">\(r_{\mathrm{s}, ik}\)</span> of <span class="math inline">\(\mathbf{r}_{\mathrm{s}, i}\)</span> is marginally standard normal at <span class="math inline">\(\theta = \theta_0\)</span> if the model is correctly specified, although its elements are not independent for members of the same family. <span class="math inline">\(\mathbf{r}_{\mathrm{c}, i}\)</span> is the vector of Cholesky residuals with elements <span class="math inline">\(r_{\mathrm{c}, ik}\)</span>, which are independent standard normal random variables at <span class="math inline">\(\theta = \theta_0\)</span> if the model is correctly specified. Note that all types of residual vectors are independent across families at <span class="math inline">\(\theta = \theta_0\)</span>.</p>
<p>A family-level goodness-of-fit statistic <span class="citation">(Hopper and Mathews 1982; Beaty, Liang, and Rao 1987)</span> can be defined from the Cholesky residual vector. The sum of the squares of these residuals for family <span class="math inline">\(i\)</span> is given by the inner product:</p>
<p><span class="math display">\[\begin{equation}
c_i^{*} = \mathbf{r}_{\mathrm{c}, i}^{'} \mathbf{r}_{\mathrm{c}, i}
= \mathbf{r}_i^{'} \left(\mathbf{L}^{-1}_i\right)^{'}
  \mathbf{L}_i^{-1} \mathbf{r}_i
= \mathbf{r}_i^{'}
  \left(\mathbf{L}_i \mathbf{L}_i^{'}\right)^{-1}
  \mathbf{r}_i
= \mathbf{r}_i^{'} \Omega_i^{-1} \mathbf{r}_i
\end{equation}\]</span></p>
<p>Because the Cholesky residuals are independent standard normal both within and between families at <span class="math inline">\(\theta = \theta_0\)</span> when the model is correct, the <span class="math inline">\(c_i^{*}\)</span> are independent <span class="math inline">\(\chi^2_{m_i-1}\)</span>, and the <span class="math inline">\(p_{c_i^{*}} = \mathrm{Pr}\left(c_i^{*} \geq \chi^2_{m_i-1}\right)\)</span> are independent standard uniform across families at <span class="math inline">\(\theta = \theta_0\)</span>.</p>
<p>Hopper and Mathews <span class="citation">(1982)</span> suggested another type of goodness-of-fit statistic to identify individuals who are outliers relative to their pedigree that we adapt to our situation. With a correctly specified model, the preceding development shows that at <span class="math inline">\(\theta = \theta_0\)</span>:</p>
<p><span class="math display">\[\begin{gather}
r_{ik} \mid \mathbf{r}_{i(-k)}, y_{i j_i}, \mathbf{X}_i, q_i, \Phi_i; \theta_0
  \sim N\left(
    \Omega_{i k (-k)} \Omega_{i (-k) (-k)}^{-1} \mathbf{r}_{i(-k)},
    \Omega^{*}_{ikk}
  \right) \\
\Omega^{*}_{ikk} = \Omega_{ikk} -
  \Omega_{i k (-k)} \Omega_{i (-k) (-k)}^{-1} \Omega_{i (-k) k}
\end{gather}\]</span></p>
<p>As a result, we define <span class="math inline">\(r^{*}_{ik} = \left( r_{ik} - \Omega_{i k (-k)} \Omega_{i (-k) (-k)}^{-1} \mathbf{r}_{i(-k)} \right) / \sqrt{\Omega^{*}_{ikk}}\)</span>, which are marginally standard normal and independent in individuals from different families, although not in individuals from the same family, at <span class="math inline">\(\theta = \theta_0\)</span> if the model is correct <span class="citation">(Hopper and Mathews 1982)</span>.</p>
<p>To take advantage of sparse matrix computations from the <code>Matrix</code> package for efficiency, we calculate residuals and related diagnostics by working with <span class="math inline">\(\mathbf{r} = \left[\mathbf{r}^{'}_1, ... \mathbf{r}^{'}_N\right]^{'}\)</span>, <span class="math inline">\(\mathbf{S} = \mathrm{diag}\left(\mathbf{S}_1, ..., \mathbf{S}_N\right)\)</span>, and <span class="math inline">\(\Omega = \mathrm{diag}\left(\Omega_1, ..., \Omega_N\right)\)</span>. Let <span class="math inline">\(\mathbf{L}\)</span> be the lower Cholesky factor of <span class="math inline">\(\Omega\)</span>. Using the <a href="http://www.netlib.org/utk/papers/factor/node9.html">block form of Cholesky factorization</a>, it follows by induction that <span class="math inline">\(\mathbf{L} = \mathrm{diag}\left(\mathbf{L}_1, ..., \mathbf{L}_N\right)\)</span>. We can then calculate Pearson-type and Cholesky residual vectors efficiently as:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{r}_{\mathrm{s}} &amp;= \mathbf{S}^{-1} \mathbf{r}
  = \begin{bmatrix}
    \mathbf{S}_1^{-1} \mathbf{r}_1 \\ ... \\ \mathbf{S}_N^{-1} \mathbf{r}_N
  \end{bmatrix}
  = \begin{bmatrix}
    \mathbf{r}_{\mathrm{s}, 1} \\ ... \\ \mathbf{r}_{\mathrm{s}, N}
  \end{bmatrix} \\
\mathbf{r}_{\mathrm{c}} &amp;= \mathbf{L}^{-1} \mathbf{r}
  = \begin{bmatrix}
    \mathbf{L}_1^{-1} \mathbf{r}_1 \\ ... \\ \mathbf{L}_N^{-1} \mathbf{r}_N
  \end{bmatrix}
  = \begin{bmatrix}
    \mathbf{r}_{\mathrm{c}, 1} \\ ... \\ \mathbf{r}_{\mathrm{c}, N}
  \end{bmatrix}
\end{split}
\end{equation}\]</span></p>
<p>A similar shortcut can be used to obtain the vector of <span class="math inline">\(r^{*}_{ik}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>. Suppose that individual <span class="math inline">\(k\)</span> in family <span class="math inline">\(i\)</span> appears in row <span class="math inline">\(l\)</span> of <span class="math inline">\(\mathbf{r}\)</span> and row and column <span class="math inline">\(l\)</span> of <span class="math inline">\(\Omega\)</span>. Let <span class="math inline">\(\mathbf{P}\)</span> be the permutation that moves row <span class="math inline">\(l\)</span> to the first row, the rows for all other non-proband family members family <span class="math inline">\(i\)</span> to rows <span class="math inline">\(2, ..., m_i - 1\)</span>, and leaves all other rows in their original order. Define:</p>
<p><span class="math display">\[\begin{equation}
\Gamma
= \mathbf{P} \Omega \mathbf{P}^{'}
= \begin{bmatrix}
    \mathbf{p}_1 \\ \mathbf{P}_2 \\ \mathbf{P}_3
  \end{bmatrix}
  \Omega
  \begin{bmatrix}
    \mathbf{p}_1^{'} &amp; \mathbf{P}_2^{'} &amp; \mathbf{P}_3^{'}
  \end{bmatrix}
= \begin{bmatrix}
    \mathbf{p}_{1} \Omega \mathbf{p}_1^{'} &amp;
      \mathbf{p}_{1} \Omega \mathbf{P}_2^{'} &amp;
      \mathbf{p}_{1} \Omega \mathbf{P}_3^{'} \\
    \mathbf{P}_{2} \Omega \mathbf{p}_1^{'} &amp;
      \mathbf{P}_{2} \Omega \mathbf{P}_2^{'} &amp;
      \mathbf{P}_{2} \Omega \mathbf{P}_3^{'} \\
    \mathbf{P}_{3} \Omega \mathbf{p}_1^{'} &amp;
      \mathbf{P}_{3} \Omega \mathbf{P}_2^{'} &amp;
      \mathbf{P}_{3} \Omega \mathbf{P}_3^{'}
  \end{bmatrix}
= \begin{bmatrix}
    \Omega_{ikk} &amp; \Omega_{ik(-k)} &amp; \mathbf{0} \\
    \Omega_{i(-k)k} &amp; \Omega_{i(-k)(-k)} &amp; \mathbf{0} \\
    \mathbf{0} &amp; \mathbf{0} &amp; \Omega_{(-i)}
  \end{bmatrix}
\end{equation}\]</span></p>
<p>the reordered covariance matrix with individual <span class="math inline">\(l\)</span> occupying element <span class="math inline">\((1, 1)\)</span> and the non-proband members of the family of individual <span class="math inline">\(l\)</span> occuping the upper left <span class="math inline">\((m_i - 1) \times (m_i - 1)\)</span> block. The inverse of this matrix can be obtained by results on block diagonal matrices and Theorem 8.5.11 of Harville <span class="citation">(1997)</span>:</p>
<p><span class="math display">\[\begin{equation}
\Gamma^{-1} =
  \begin{bmatrix}
    1 / \Omega_{ikk}^{*} &amp;
      -\Omega_{ik(-k)} \Omega_{i(-k)(-k)}^{-1}  / \Omega_{ikk}^{*} &amp;
      \mathbf{0} \\
    -\Omega_{i(-k)(-k)}^{-1} \Omega_{i(-k)k}  / \Omega_{ikk}^{*} &amp;
      \Omega_{i(-k)(-k)}^{-1} +
        \Omega_{i(-k)(-k)}^{-1} \Omega_{i(-k)k} \Omega_{ik(-k)}
        \Omega_{i(-k)(-k)}^{-1} / \Omega_{ikk}^{*} &amp;
      \mathbf{0} \\
    \mathbf{0} &amp; \mathbf{0} &amp; \Omega_{(-i)}^{-1}
  \end{bmatrix}
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(\Gamma^{-1} = \mathbf{P} \Omega^{-1} \mathbf{P}^{'}\)</span>, we also have that:</p>
<p><span class="math display">\[\begin{equation}
\Omega^{-1} \mathbf{r}
= \mathbf{P}^{'} \mathbf{P} \Omega^{-1} \mathbf{P}^{'} \mathbf{P} \mathbf{r}
= \mathbf{P}^{'} \Gamma^{-1} \mathbf{P} \mathbf{r}
= \mathbf{P}^{'} \Gamma^{-1}
  \begin{bmatrix}
    r_{ik} \\ \mathbf{r}_{i(-k)} \\ \mathbf{r}_{(-i)}
  \end{bmatrix}
= \mathbf{P}^{'}
  \begin{bmatrix}
    \frac{
      r_{ik} - \Omega_{ik(-k)} \Omega_{i(-k)(-k)}^{-1} \mathbf{r}_{i(-k)}
    }{ \Omega_{ikk}^{*} } \\
    ...
   \end{bmatrix}
= \mathbf{P}^{'}
  \begin{bmatrix}
    \frac{r^{*}_{ik}}{\sqrt{\Omega_{ikk}^{*}}} \\
    ...
   \end{bmatrix}
\end{equation}\]</span></p>
<p>This implies that row <span class="math inline">\(l\)</span> of <span class="math inline">\(\Omega^{-1} \mathbf{r}\)</span> contains <span class="math inline">\(r^{*}_{ik} / \sqrt{\Omega_{ikk}^{*}}\)</span>, the goodness-of-fit statistic for the individual in row <span class="math inline">\(l\)</span> of <span class="math inline">\(\mathbf{r}\)</span> scaled by the square root of the individual’s variance. The preceding development also shows that <span class="math inline">\(1 / \Omega_{ikk}^{*}\)</span> can be found in element <span class="math inline">\(l\)</span> of the diagonal of <span class="math inline">\(\Omega^{-1}\)</span> because <span class="math inline">\(\mathbf{p}_{1} \Omega^{-1} \mathbf{p}_1^{'} = 1 / \Omega_{ikk}^{*}\)</span>. Letting <span class="math inline">\(\mathbf{T} = \mathrm{diag}\left(\left[\left[\sqrt{\Omega^{-1}_{ikk}}\right]_{k = 1}^{m_i - 1}\right]_{i = 1}^{N}\right)\)</span>, we can obtain the vector of <span class="math inline">\(r^{*}_{ik}\)</span> with indexes in the same order as <span class="math inline">\(\mathbf{r}\)</span> as:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{r}^* =
  \mathbf{T}^{-1} \Omega^{-1} \mathbf{r}
\end{equation}\]</span></p>
<p>which involves a single inversion of a sparse block diagonal matrix and a single solution of a sparse linear system.</p>
<p>In practice, the unknown <span class="math inline">\(\theta_0\)</span> is estimated using the same data by maximum likelihood and estimates <span class="math inline">\(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta})\)</span> of <span class="math inline">\(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \theta_0)\)</span> are used. These estimates are designated by a superscript “hat” in the documentation–that is <span class="math inline">\(\hat{\mathbf{r}}_{c,i}\)</span> is <span class="math inline">\(\mathbf{r}_{c,i}\)</span> evaluated at <span class="math inline">\(\theta = \hat{\theta}\)</span>. We now consider the distributional properties of a generic <span class="math inline">\(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta})\)</span>. Because <span class="math inline">\(\hat{\theta} \overset{p}{\longrightarrow} \theta_0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span> even if <span class="math inline">\(\theta_0\)</span> is on the boundary of the parameter space (see <a href="#inference">Inference</a>) and <span class="math inline">\(\mathbf{y}_{\mathrm{np}, i} \overset{p}{\longrightarrow} \mathbf{y}_{\mathrm{np}, i}\)</span> trivially, they converge in probability jointly <span class="citation">(Van der Vaart 2007, Theorem 2.7(vi))</span>. We can then apply the Continuous Mapping Theorem <span class="citation">(Van der Vaart 2007)</span> to conclude that <span class="math inline">\(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta}) \overset{p}{\longrightarrow} h_{i}(\mathbf{y}_{\mathrm{np}, i}, \theta_0)\)</span> for all <span class="math inline">\(i\)</span>. It immediately follows that <span class="math inline">\(\left(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta}), h_{i^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \hat{\theta})\right) \overset{p}{\longrightarrow} \left(h_{i}(\mathbf{y}_{\mathrm{np}, i}, \theta_0), h_{i^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \theta_0)\right)\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(i^{'} \neq i\)</span>. Note that the distribution of <span class="math inline">\(\mathbf{y}_{\mathrm{np}, i}\)</span> is always (\ref{eq:conddist}) with <span class="math inline">\(\theta = \theta_0\)</span> in the preceding results and all subsequent development.</p>
<p>Let <span class="math inline">\(k \in \{1, ..., K_i\}\)</span> index the components of <span class="math inline">\(h_{i}\)</span> in a given family. Note that, for diagnostics such as <span class="math inline">\(\hat{p}_{\hat{c}^{*}_i}\)</span>, <span class="math inline">\(K_i \equiv 1\)</span> for all <span class="math inline">\(i\)</span>, but <span class="math inline">\(K_i = m_i - 1\)</span> for residuals. Now denote the marginal cumulative distribution function (CDF) of <span class="math inline">\(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta})\)</span> in a sample of <span class="math inline">\(N\)</span> families by <span class="math inline">\(F_{ik, N}(t)\)</span> and the marginal CDF of <span class="math inline">\(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \theta_0)\)</span> by <span class="math inline">\(F_{ik}(t)\)</span>. When each <span class="math inline">\(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \theta_0)\)</span> has the same continuous marginal distribution <span class="math inline">\(F_{ik}(t) \equiv F(t)\)</span>, it follows from the convergence in probability results given above that <span class="math inline">\(\underset{t}{\sup} \left| F_{ik, N}(t) - F(t) \right| \rightarrow 0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span> for all <span class="math inline">\(ik\)</span>. Considering <span class="math inline">\(\left(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta}), h_{i^{'} k^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \hat{\theta}) \right)\)</span> for any <span class="math inline">\(ik\)</span> and <span class="math inline">\(i^{'} k^{'} \neq ik\)</span>, their joint convergence in probability to <span class="math inline">\(\left(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \theta_0), h_{i^{'} k^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \theta_0) \right)\)</span> follows immediately from the first convergence in probability result above when <span class="math inline">\(i^{'} = i\)</span> and the second otherwise. Denoting the joint CDF of <span class="math inline">\(\left(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta}), h_{i^{'} k^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \hat{\theta}) \right)\)</span> in a sample of <span class="math inline">\(N\)</span> families by <span class="math inline">\(F_{ik, i^{'} k^{'}, N}(s, t)\)</span> and the joint CDF of <span class="math inline">\(\left(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \theta_0), h_{i^{'} k^{'}}(\mathbf{y}_{\mathrm{np}, i^{'}}, \theta_0) \right)\)</span> by <span class="math inline">\(F_{ik, i^{'} k^{'}}(s, t)\)</span>, it follows that <span class="math inline">\(\underset{s, t}{\sup} \left| F_{ik, i^{'} k^{'}, N}(s, t) - F_{ik, i^{'} k^{'}}(s, t) \right| \rightarrow 0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span> for all <span class="math inline">\(ik\)</span> and <span class="math inline">\(i^{'} k^{'} \neq ik\)</span>.</p>
<p>Define the empirical distribution function (EDF) of <span class="math inline">\(h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta})\)</span> in the sample as <span class="math inline">\(\hat{F}_{N}(t) = m^{-1} \sum_{i = 1}^N \sum_{k=1}^{K_i} I\left[h_{ik}(\mathbf{y}_{\mathrm{np}, i}, \hat{\theta}) \leq t\right]\)</span>, where <span class="math inline">\(m = \sum_{i=1}^N K_i\)</span>. Based on the convergence in distribution shown above, for any <span class="math inline">\(\varepsilon &gt; 0\)</span>, we can choose an <span class="math inline">\(N_{\varepsilon}\)</span> such that <span class="math inline">\(\underset{ik, t}{\sup} \left| F_{ik, N}(t) - F(t) \right| &lt; \varepsilon\)</span> and <span class="math inline">\(\underset{ik, i^{'} k^{'} \neq ik, s, t}{\sup} \left| F_{ik, i^{'} k^{'}, N}(s, t) - F_{ik, i^{'} k^{'}}(s, t) \right| &lt; \varepsilon\)</span> whenever <span class="math inline">\(N \geq N_{\varepsilon}\)</span>. For an arbitrary <span class="math inline">\(N \geq N_{\varepsilon}\)</span> and <span class="math inline">\(t\)</span>, we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\left| \mathrm{E}\left[\hat{F}_{N}(t)\right] - F(t) \right|
&amp;= \left|  m^{-1} \sum_{i = 1}^N \sum_{k=1}^{K_i}
  \left(F_{ik, N}(t) - F(t)\right) \right| \\
&amp;\leq m^{-1} \sum_{i = 1}^N \sum_{k=1}^{K_i}
  \left| F_{ik, N}(t) - F(t) \right| \\
&amp;&lt; \varepsilon
\end{split}
\end{equation}\]</span></p>
<p>which implies that <span class="math inline">\(\mathrm{E}\left[\hat{F}_{N}(t)\right] \rightarrow F(t)\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>. We also have that:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp;\left| \mathrm{Var}\left(\hat{F}_{N}(t)\right) -
  m^{-1} \left(F(t) - F(t)^2 \right) -
  m^{-2} \sum_{ik} \sum_{i^{'} k^{'} \neq ik}
  \left(F_{ik, i^{'} k^{'}}(t, t) - F(t)^2 \right)
\right| \\
&amp;\phantom{0000} \leq \left|
  m^{-2} \sum_{ik} \left(F_{ik,N}(t) - F(t) + F(t)^2 - F_{ik, N}(t)^2\right)
\right| \\
&amp;\phantom{000000} + \left|
  m^{-2} \sum_{ik} \sum_{i^{'} k^{'} \neq ik}
  \left(F_{ik, i^{'} k^{'}, N}(t, t) - F_{ik, i^{'} k^{'}}(t, t) +
  F(t)^2 - F_{ik, N}(t)F_{i^{'} k^{'}, N}(t)\right)
\right| \\
&amp;\phantom{0000} \leq m^{-2} \sum_{ik} \left| F_{ik, N}(t) - F(t) \right| +
  m^{-2} \sum_{ik} \left| F(t)^2 - F_{ik, N}(t)^2 \right| \\
&amp;\phantom{000000} + m^{-2} \sum_{ik} \sum_{i^{'} k^{'} \neq ik}
  \left|F_{ik, i^{'} k^{'}, N}(t, t) - F_{ik, i^{'} k^{'}}(t, t)\right| +
  m^{-2} \sum_{ik} \sum_{i^{'} k^{'}
  \neq ik} \left|F(t)^2 - F_{ik, N}(t)F_{i^{'} k^{'}, N}(t)\right| \\
&amp;\phantom{0000} &lt; m^{-1} \varepsilon +
  m^{-2} \sum_{ik} \left| F(t) - F_{ik, N}(t) \right|\left| F(t) +
  F_{ik, N}(t) \right| \\
&amp;\phantom{000000} +  \left(1 - m^{-1}\right) \varepsilon +
  m^{-2} \sum_{ik} \sum_{i^{'} k^{'} \neq ik}
  \left|F(t)\right|\left|F(t) - F_{ik, N}(t)\right| +
  m^{-2} \sum_{ik} \sum_{i^{'} k^{'} \neq ik}
  \left|F_{ik, N}(t)\right|\left|F(t) - F_{i^{'} k^{'}, N}(t)\right| \\
&amp;\phantom{0000} &lt; \varepsilon + 2 m^{-1} \varepsilon +
  \left(1 - m^{-1}\right) \varepsilon + \left(1 - m^{-1}\right) \varepsilon
  = 3 \varepsilon
\end{split}
\end{equation}\]</span></p>
<p>For quantities that are pairwise independent with identical marginal distributions at <span class="math inline">\(\theta = \theta_0\)</span>, such as <span class="math inline">\(r_{c,ik}\)</span> and <span class="math inline">\(p_{c^{*}_i}\)</span>, <span class="math inline">\(F_{ik, i^{'} k^{'}}(t, t) = F(t)^2\)</span>, <span class="math inline">\(\sum_{ik} \sum_{i^{'} k^{'} \neq ik} \left(F_{ik, i^{'} k^{'}}(t, t) - F(t)^2 \right) \equiv 0\)</span>, and it follows from the above results that <span class="math inline">\(\mathrm{Var}\left(\hat{F}_{N}(t)\right) \rightarrow 0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>. In the case of <span class="math inline">\(r^{*}_{ik}\)</span>, the pairs are dependent at <span class="math inline">\(\theta = \theta_0\)</span> for individuals in the same family but indpendent for unrelated individuals. Note that all non-zero terms in <span class="math inline">\(\sum_{ik} \sum_{i^{'} k^{'} \neq ik} \left(F_{ik, i^{'} k^{'}}(t, t) - F(t)^2 \right)\)</span> are in <span class="math inline">\(\left[-1, 1\right]\)</span>, so <span class="math inline">\(m^{-2}\)</span> times this quantity is bounded by <span class="math inline">\([-m^{-2} m_{+}, m^{-2} m_{+}]\)</span>, where <span class="math inline">\(m_{+}\)</span> is the number of non-zero terms. The block diagonal covariance structure of <span class="math inline">\(\mathbf{r}^{*}\)</span> implies that <span class="math inline">\(m_{+} = \sum_{i=1}^{N} K_i^2 - m\)</span>. Because <span class="math inline">\(m \geq N\)</span>, if family sizes are bounded by a constant <span class="math inline">\(K\)</span>, then <span class="math inline">\(m^{-2} m_{+} \leq N^{-1} (K^2 - 1) \rightarrow 0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>. As a result, <span class="math inline">\(\mathrm{Var}\left(\hat{F}_{N}(t)\right) \rightarrow 0\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span> in this case as well. It then follows from Chebyshev’s Inequality that <span class="math inline">\(\hat{F}_{N}(t) \overset{p}{\longrightarrow} F(t)\)</span> at each <span class="math inline">\(t\)</span> <span class="citation">(Greene 2003)</span>.</p>
<p>The consistency of the EDF in these cases also implies the consistency of the quantile function <span class="citation">(Van der Vaart 2007)</span>. As a result, goodness of fit can be assessed visually using plots of the EDF or a quantile-quantile (QQ) plot of the estimated quantities against the corresponding quantiles of the theoretical marginal distribution. For example, Cholesky residuals can be plotted against the corresponding theoretical quantiles for a sample of the same size from the standard normal distribution. While the results above demonstrate consistency, they provide no insights into sampling variability. In fact, neither standard confidence limits for QQ plots nor standard goodness-of-fit statistics correctly reflect the sampling variability of the EDF of these estimated quantities because they do not account for estimation of parameters from the same data. In the case of Cholesky residuals, Houseman, Ryan, and Coull <span class="citation">(2004)</span> established that the scaled difference between the empirical distribution function of the estimated Cholesky residuals and the standard normal CDF converges pointwise to a mean-zero normal random variable with variance differing from the standard <span class="math inline">\(\Phi\left(t\right)\left(1 - \Phi\left(t\right)\right)\)</span>. The authors note that this is a consequence of shrinkage due to the use of estimated parameter, which also squares with the observation of Beaty, Liang, and Rao <span class="citation">(1987)</span> that the sum of the estimated family chi-square statistics, which is the sum of the squared estimated Cholesky residuals, must equal the number of non-probands, <span class="math inline">\(m\)</span>. With known <span class="math inline">\(\theta = \theta_0\)</span>, values, this sum would be a <span class="math inline">\(\chi^2_m\)</span> random variable with mean <span class="math inline">\(m\)</span> and variance <span class="math inline">\(2m\)</span>, but shrinkage due to use of <span class="math inline">\(\theta = \hat{\theta}\)</span> reduces the variance of the sum to zero. In summary, while the pointwise convergence in probability of the EDF to the CDF still holds for estimated residuals and diagnostics and allows for comparison with the identity line on the QQ plot, unmodified confidence limits or goodness-of-fit tests based on the EDF may yield misleading inferences. For Cholesky residuals, the method of Houseman and colleagues can be used to estimate the correct pointwise sampling variance for the EDF.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-beaty1987">
<p>Beaty, T. H., K. Y. Liang, and D. C. Rao. 1987. “Robust Inference for Variance Components Models in Families Ascertained Through Probands: I. Conditioning on Proband’s Phenotype.” <em>Genetic Epidemiology</em> 4 (3): 203–10. <a href="https://doi.org/10.1002/gepi.1370040305">https://doi.org/10.1002/gepi.1370040305</a>.</p>
</div>
<div id="ref-belsley1980">
<p>Belsley, David A. 1980. “On the Efficient Computation of the Nonlinear Full-Information Maximum-Likelihood Estimator.” <em>Journal of Econometrics</em> 14 (2): 203–25. <a href="https://doi.org/10.1016/0304-4076(80)90091-3">https://doi.org/10.1016/0304-4076(80)90091-3</a>.</p>
</div>
<div id="ref-boerwinkle1986">
<p>Boerwinkle, E., R. Chakraborty, and C. F. Sing. 1986. “The Use of Measured Genotype Information in the Analysis of Quantitative Phenotypes in Man.” <em>Annals of Human Genetics</em> 50 (2): 181–94. <a href="https://doi.org/10.1111/j.1469-1809.1986.tb01037.x">https://doi.org/10.1111/j.1469-1809.1986.tb01037.x</a>.</p>
</div>
<div id="ref-fletcher2000">
<p>Fletcher, David. 2000. <em>Practical Methods of Optimization</em>. 1st ed. John Wiley &amp; Sons, Ltd. <a href="https://doi.org/10.1002/9781118723203">https://doi.org/10.1002/9781118723203</a>.</p>
</div>
<div id="ref-greene2003">
<p>Greene, William H. 2003. <em>Econometric Analysis</em>. 5th ed. Upper Saddle River, NJ: Prentice Hall.</p>
</div>
<div id="ref-harville1997">
<p>Harville, David A. 1997. <em>Matrix Algebra from a Statistician’s Perspective</em>. New York: Springer.</p>
</div>
<div id="ref-hopper1982">
<p>Hopper, J. L., and J. D. Mathews. 1982. “Extensions to Multivariate Normal Models for Pedigree Analysis.” <em>Annals of Human Genetics</em> 46 (4): 373–83. <a href="https://doi.org/10.1111/j.1469-1809.1982.tb01588.x">https://doi.org/10.1111/j.1469-1809.1982.tb01588.x</a>.</p>
</div>
<div id="ref-houseman2004">
<p>Houseman, E. Andrés, Louise M Ryan, and Brent A Coull. 2004. “Cholesky Residuals for Assessing Normal Errors in a Linear Model with Correlated Outcomes.” <em>Journal of the American Statistical Association</em> 99 (466): 383–94. <a href="https://doi.org/10.1198/016214504000000403">https://doi.org/10.1198/016214504000000403</a>.</p>
</div>
<div id="ref-lange2002">
<p>Lange, Kenneth. 2002. <em>Mathematical and Statistical Methods for Genetic Analysis</em>. 2nd ed. Statistics for Biology and Health. New York: Springer.</p>
</div>
<div id="ref-nocedal2006">
<p>Nocedal, Jorge, and Stephen J. Wright. 2006. <em>Numerical Optimization</em>. 2nd ed. Springer Series in Operations Research. New York: Springer.</p>
</div>
<div id="ref-self1987">
<p>Self, Steven G., and Kung-Yee Liang. 1987. “Asymptotic Properties of Maximum Likelihood Estimators and Likelihood Ratio Tests Under Nonstandard Conditions.” <em>Journal of the American Statistical Association</em> 82 (398): 605–10. <a href="https://doi.org/10.2307/2289471">https://doi.org/10.2307/2289471</a>.</p>
</div>
<div id="ref-vandervaart2007">
<p>Van der Vaart, A. W. 2007. <em>Asymptotic Statistics</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-zhu1997">
<p>Zhu, Ciyou, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. 1997. “Algorithm 778: L-BFGS-B: Fortran Subroutines for Large-Scale Bound-Constrained Optimization.” <em>ACM Transactions on Mathematical Software</em> 23 (4): 550–60. <a href="https://doi.org/10.1145/279232.279236">https://doi.org/10.1145/279232.279236</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Daniel D. Kinnamon.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
